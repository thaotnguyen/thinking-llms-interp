# python generate_responses.py --model meta-llama/Llama-3.1-8B --save_every 1 --max_tokens 1000 --is_base_model
# python generate_responses.py --model deepseek-ai/DeepSeek-R1-Distill-Llama-8B --save_every 1 --max_tokens 1000

# python generate_responses.py --model qwen/Qwen2.5-Math-1.5B --save_every 1 --max_tokens 1000 --is_base_model
# python generate_responses.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --save_every 1 --max_tokens 1000

# python generate_responses.py --model qwen/Qwen2.5-14B --save_every 1 --max_tokens 1000 --is_base_model
# python generate_responses.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --save_every 1 --max_tokens 1000

# python generate_responses.py --model qwen/Qwen2.5-32B --save_every 1 --max_tokens 1000 --is_base_model
# python generate_responses.py --model Qwen/QwQ-32B --save_every 1 --max_tokens 1000 --engine vllm

# python generate_responses.py --model Qwen/Qwen2.5-32B --save_every 1 --max_tokens 1000 --is_base_model
python generate_responses.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --save_every 1 --max_tokens 1000

# python generate_responses.py --model meta-llama/Llama-3.3-70B-Instruct --save_every 1 --max_tokens 1000 --is_base_model
# python generate_responses.py --model deepseek-ai/DeepSeek-R1-Distill-Llama-70B --save_every 1 --max_tokens 1000